{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q segmentation_models_pytorch\n!pip install -q monai\n","metadata":{"execution":{"iopub.status.busy":"2022-08-31T18:59:59.607863Z","iopub.execute_input":"2022-08-31T18:59:59.608432Z","iopub.status.idle":"2022-08-31T19:00:34.093372Z","shell.execute_reply.started":"2022-08-31T18:59:59.608369Z","shell.execute_reply":"2022-08-31T19:00:34.091947Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting segmentation_models_pytorch\n  Downloading segmentation_models_pytorch-0.3.0-py3-none-any.whl (97 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m350.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting pretrainedmodels==0.7.4\n  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting efficientnet-pytorch==0.7.1\n  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages (from segmentation_models_pytorch) (9.1.1)\nRequirement already satisfied: torchvision>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from segmentation_models_pytorch) (0.12.0+cpu)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from segmentation_models_pytorch) (4.64.0)\nCollecting timm==0.4.12\n  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.11.0+cpu)\nRequirement already satisfied: munch in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels==0.7.4->segmentation_models_pytorch) (2.5.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (4.3.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (1.21.6)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (2.28.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from munch->pretrainedmodels==0.7.4->segmentation_models_pytorch) (1.15.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (3.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (1.26.11)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (2.1.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (2022.6.15)\nBuilding wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16446 sha256=07873f5fc175c14c668dd6bca6766189c61e02375cce42979ac6f2df5d62ceb8\n  Stored in directory: /root/.cache/pip/wheels/0e/cc/b2/49e74588263573ff778da58cc99b9c6349b496636a7e165be6\n  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60965 sha256=70bf18b9552c4f7d032244fe36fd4636b7d55ee7f6ba5a89cf1cd6ac74a7d47f\n  Stored in directory: /root/.cache/pip/wheels/ed/27/e8/9543d42de2740d3544db96aefef63bda3f2c1761b3334f4873\nSuccessfully built efficientnet-pytorch pretrainedmodels\nInstalling collected packages: efficientnet-pytorch, timm, pretrainedmodels, segmentation_models_pytorch\nSuccessfully installed efficientnet-pytorch-0.7.1 pretrainedmodels-0.7.4 segmentation_models_pytorch-0.3.0 timm-0.4.12\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting monai\n  Downloading monai-0.9.1-202207251608-py3-none-any.whl (990 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.7/990.7 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from monai) (1.21.6)\nRequirement already satisfied: torch>=1.7 in /opt/conda/lib/python3.7/site-packages (from monai) (1.11.0+cpu)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.7->monai) (4.3.0)\nInstalling collected packages: monai\nSuccessfully installed monai-0.9.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -q wandb","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nfrom torch.utils.data import DataLoader, random_split\nimport albumentations as A\n\nimport segmentation_models_pytorch as smp\n# import torchsummary\n\nimport pandas as pd\nimport numpy as np\nimport random, shutil, time, os\n\nimport sklearn\nimport cv2\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\nimport albumentations as A\n\nfrom glob import glob\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import KFold, GroupKFold\nfrom sklearn.metrics import roc_auc_score\n# from skimage import color\nfrom IPython import display as ipd\n\nimport scipy\nimport pdb\nimport gc\n\nimport monai\nimport tifffile as tiff\n\nfrom torch.cuda import amp\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint('done')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-31T19:00:34.095661Z","iopub.execute_input":"2022-08-31T19:00:34.096472Z","iopub.status.idle":"2022-08-31T19:00:38.493789Z","shell.execute_reply.started":"2022-08-31T19:00:34.096429Z","shell.execute_reply":"2022-08-31T19:00:38.492393Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"done\n","output_type":"stream"}]},{"cell_type":"code","source":"sz = 256   #the size of tiles\nreduce = 4 #reduce the original images by 4 times \n\nBASE_DIR = '../input/hubmap-organ-segmentation'\nTRAIN = True\nif TRAIN:\n    DATA_DIR = os.path.join(BASE_DIR, 'train_images')\nelse:\n    DATA_DIR = os.path.join(BASE_DIR, 'test_images')\n            \ndf = pd.read_csv(os.path.join(BASE_DIR, 'train.csv'))\ndf['path'] = df['id'].apply(lambda fname : os.path.join(DATA_DIR, str(fname) + '.tiff'))\norgan_to_class = {\n    'prostate':0,\n    'spleen':1,\n    'lung':2,\n    'kidney':3,\n    'largeintestine':4\n}\ndf['classes'] = df['organ'].apply(lambda organ : organ_to_class[organ])\ndf.head(5)\n","metadata":{"execution":{"iopub.status.busy":"2022-08-31T19:38:24.862380Z","iopub.execute_input":"2022-08-31T19:38:24.862849Z","iopub.status.idle":"2022-08-31T19:38:25.054197Z","shell.execute_reply.started":"2022-08-31T19:38:24.862812Z","shell.execute_reply":"2022-08-31T19:38:25.053311Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"      id     organ data_source  img_height  img_width  pixel_size  \\\n0  10044  prostate         HPA        3000       3000         0.4   \n1  10274  prostate         HPA        3000       3000         0.4   \n2  10392    spleen         HPA        3000       3000         0.4   \n3  10488      lung         HPA        3000       3000         0.4   \n4  10610    spleen         HPA        3000       3000         0.4   \n\n   tissue_thickness                                                rle   age  \\\n0                 4  1459676 77 1462675 82 1465674 87 1468673 92 14...  37.0   \n1                 4  715707 2 718705 8 721703 11 724701 18 727692 3...  76.0   \n2                 4  1228631 20 1231629 24 1234624 40 1237623 47 12...  82.0   \n3                 4  3446519 15 3449517 17 3452514 20 3455510 24 34...  78.0   \n4                 4  478925 68 481909 87 484893 105 487863 154 4908...  21.0   \n\n      sex                                               path  classes  \n0    Male  ../input/hubmap-organ-segmentation/train_image...        0  \n1    Male  ../input/hubmap-organ-segmentation/train_image...        0  \n2    Male  ../input/hubmap-organ-segmentation/train_image...        1  \n3    Male  ../input/hubmap-organ-segmentation/train_image...        2  \n4  Female  ../input/hubmap-organ-segmentation/train_image...        1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>organ</th>\n      <th>data_source</th>\n      <th>img_height</th>\n      <th>img_width</th>\n      <th>pixel_size</th>\n      <th>tissue_thickness</th>\n      <th>rle</th>\n      <th>age</th>\n      <th>sex</th>\n      <th>path</th>\n      <th>classes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10044</td>\n      <td>prostate</td>\n      <td>HPA</td>\n      <td>3000</td>\n      <td>3000</td>\n      <td>0.4</td>\n      <td>4</td>\n      <td>1459676 77 1462675 82 1465674 87 1468673 92 14...</td>\n      <td>37.0</td>\n      <td>Male</td>\n      <td>../input/hubmap-organ-segmentation/train_image...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10274</td>\n      <td>prostate</td>\n      <td>HPA</td>\n      <td>3000</td>\n      <td>3000</td>\n      <td>0.4</td>\n      <td>4</td>\n      <td>715707 2 718705 8 721703 11 724701 18 727692 3...</td>\n      <td>76.0</td>\n      <td>Male</td>\n      <td>../input/hubmap-organ-segmentation/train_image...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10392</td>\n      <td>spleen</td>\n      <td>HPA</td>\n      <td>3000</td>\n      <td>3000</td>\n      <td>0.4</td>\n      <td>4</td>\n      <td>1228631 20 1231629 24 1234624 40 1237623 47 12...</td>\n      <td>82.0</td>\n      <td>Male</td>\n      <td>../input/hubmap-organ-segmentation/train_image...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10488</td>\n      <td>lung</td>\n      <td>HPA</td>\n      <td>3000</td>\n      <td>3000</td>\n      <td>0.4</td>\n      <td>4</td>\n      <td>3446519 15 3449517 17 3452514 20 3455510 24 34...</td>\n      <td>78.0</td>\n      <td>Male</td>\n      <td>../input/hubmap-organ-segmentation/train_image...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10610</td>\n      <td>spleen</td>\n      <td>HPA</td>\n      <td>3000</td>\n      <td>3000</td>\n      <td>0.4</td>\n      <td>4</td>\n      <td>478925 68 481909 87 484893 105 487863 154 4908...</td>\n      <td>21.0</td>\n      <td>Female</td>\n      <td>../input/hubmap-organ-segmentation/train_image...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def rle_encode(img):\n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels = img.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\n\ndef rle_decode(mask_rle, wid, hei):\n    shape = (wid, hei)\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape).T\n","metadata":{"execution":{"iopub.status.busy":"2022-08-31T19:35:03.409614Z","iopub.execute_input":"2022-08-31T19:35:03.409981Z","iopub.status.idle":"2022-08-31T19:35:03.421010Z","shell.execute_reply.started":"2022-08-31T19:35:03.409953Z","shell.execute_reply":"2022-08-31T19:35:03.419510Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"s_th = 40  #saturation blancking threshold\np_th = 1000*(sz//256)**2 #threshold for the minimum number of pixels\n\n\nclass HuBMAPDataset(torch.utils.data.Dataset):\n    def __init__(self, idx, sz=sz, reduce=reduce, encs=None):\n        self.data = tiff.imread(os.path.join(DATA_DIR,str(idx)+'.tiff'))\n        # some images have issues with their format \n        # and must be saved correctly before reading with rasterio\n        if self.data.count != 3:\n            subdatasets = self.data.subdatasets\n            self.layers = []\n            if len(subdatasets) > 0:\n                for i, subdataset in enumerate(subdatasets, 0):\n                    self.layers.append(rasterio.open(subdataset))\n        self.shape = self.data.shape\n        self.reduce = reduce\n        self.sz = reduce*sz\n        self.pad0 = (self.sz - self.shape[0]%self.sz)%self.sz\n        self.pad1 = (self.sz - self.shape[1]%self.sz)%self.sz\n        self.n0max = (self.shape[0] + self.pad0)//self.sz\n        self.n1max = (self.shape[1] + self.pad1)//self.sz\n        self.mask = enc2mask(encs,(self.shape[1],self.shape[0])) if encs is not None else None\n        \n    def __len__(self):\n        return self.n0max*self.n1max\n    \n    def __getitem__(self, idx):\n        # the code below may be a little bit difficult to understand,\n        # but the thing it does is mapping the original image to\n        # tiles created with adding padding (like in the previous version of the kernel)\n        # then the tiles are loaded with rasterio\n        # n0,n1 - are the x and y index of the tile (idx = n0*self.n1max + n1)\n        n0,n1 = idx//self.n1max, idx%self.n1max\n        # x0,y0 - are the coordinates of the lower left corner of the tile in the image\n        # negative numbers correspond to padding (which must not be loaded)\n        x0,y0 = -self.pad0//2 + n0*self.sz, -self.pad1//2 + n1*self.sz\n\n        # make sure that the region to read is within the image\n        p00,p01 = max(0,x0), min(x0+self.sz,self.shape[0])\n        p10,p11 = max(0,y0), min(y0+self.sz,self.shape[1])\n        img = np.zeros((self.sz,self.sz,3),np.uint8)\n        mask = np.zeros((self.sz,self.sz),np.uint8)\n        # mapping the loade region to the tile\n        if self.data.count == 3:\n            img[(p00-x0):(p01-x0),(p10-y0):(p11-y0)] = np.moveaxis(self.data.read([1,2,3],\n                window=Window.from_slices((p00,p01),(p10,p11))), 0, -1)\n        else:\n            for i,layer in enumerate(self.layers):\n                img[(p00-x0):(p01-x0),(p10-y0):(p11-y0),i] =\\\n                  layer.read(1,window=Window.from_slices((p00,p01),(p10,p11)))\n        if self.mask is not None: mask[(p00-x0):(p01-x0),(p10-y0):(p11-y0)] = self.mask[p00:p01,p10:p11]\n        \n        if self.reduce != 1:\n            img = cv2.resize(img,(self.sz//reduce,self.sz//reduce),\n                             interpolation = cv2.INTER_AREA)\n            mask = cv2.resize(mask,(self.sz//reduce,self.sz//reduce),\n                             interpolation = cv2.INTER_NEAREST)\n        #check for empty imges\n        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n        h,s,v = cv2.split(hsv)\n        #return -1 for empty images\n        return img, mask, (-1 if (s>s_th).sum() <= p_th or img.sum() <= p_th else idx)\n","metadata":{"execution":{"iopub.status.busy":"2022-08-31T19:38:52.664780Z","iopub.execute_input":"2022-08-31T19:38:52.665352Z","iopub.status.idle":"2022-08-31T19:38:52.686773Z","shell.execute_reply.started":"2022-08-31T19:38:52.665295Z","shell.execute_reply":"2022-08-31T19:38:52.685456Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"for index, encs in tqdm(df.iterrows()):\n    print(encs['id'])\n    break","metadata":{"execution":{"iopub.status.busy":"2022-08-31T19:46:13.767786Z","iopub.execute_input":"2022-08-31T19:46:13.768215Z","iopub.status.idle":"2022-08-31T19:46:13.825681Z","shell.execute_reply.started":"2022-08-31T19:46:13.768182Z","shell.execute_reply":"2022-08-31T19:46:13.824314Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"345930a6eb9e403ea8cd81b08fbb6120"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_17/634293199.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mencs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str"],"ename":"TypeError","evalue":"tuple indices must be integers or slices, not str","output_type":"error"}]},{"cell_type":"code","source":"x_tot,x2_tot = [],[]\n\nfor index, encs in tqdm(df.iterrows()):\n    #image+mask dataset\n    ds = HuBMAPDataset(encs['id'],encs=encs)\n    for i in range(len(ds)):\n        im,m,idx = ds[i]\n        if idx < 0: continue\n                \n        x_tot.append((im/255.0).reshape(-1,3).mean(0))\n        x2_tot.append(((im/255.0)**2).reshape(-1,3).mean(0))\n            \n        #write data   \n        im = cv2.imencode('.png',cv2.cvtColor(im, cv2.COLOR_RGB2BGR))[1]\n        plt.subplots()\n        plt.imshow(im)\n        \n        m = cv2.imencode('.png',m)[1]\n        plt.subplots()\n        plt.imshow(m)\n        \n#image stats\nimg_avr =  np.array(x_tot).mean(0)\nimg_std =  np.sqrt(np.array(x2_tot).mean(0) - img_avr**2)\nprint('mean:',img_avr, ', std:', img_std)\n","metadata":{"execution":{"iopub.status.busy":"2022-08-31T19:46:20.227562Z","iopub.execute_input":"2022-08-31T19:46:20.227987Z","iopub.status.idle":"2022-08-31T19:46:20.942532Z","shell.execute_reply.started":"2022-08-31T19:46:20.227950Z","shell.execute_reply":"2022-08-31T19:46:20.941470Z"},"trusted":true},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc0efcedfb784fe99f388bdfeb9a5d01"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_17/2302839272.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#image+mask dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHuBMAPDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_17/572171360.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, idx, sz, reduce, encs)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# some images have issues with their format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# and must be saved correctly before reading with rasterio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0msubdatasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'count'"],"ename":"AttributeError","evalue":"'numpy.ndarray' object has no attribute 'count'","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
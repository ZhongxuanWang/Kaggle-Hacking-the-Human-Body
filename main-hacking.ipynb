{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# https://www.kaggle.com/code/awsaf49/uwmgi-unet-train-pytorch\n!pip install -q wandb\n\n!pip install -q ../input/monai/monai-0.9.1-202207251608-py3-none-any.whl\n\n!pip install -q ../input/lib-pretrainedmodels/pretrainedmodels-0.7.4-py3-none-any.whl\n!pip install -q ../input/tim-0412/timm-0.4.12-py3-none-any.whl\n!pip install -q ../input/lib-efficientnet-pytorch/efficientnet_pytorch-0.7.1-py3-none-any.whl\n!pip install -q ../input/segmentation-models-pytorch/segmentation_models_pytorch-0.3.0-py3-none-any.whl\n\n# !pip install -q scikit-image\nfrom kaggle_datasets import KaggleDatasets","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-09-02T16:37:32.620707Z","iopub.execute_input":"2022-09-02T16:37:32.621158Z","iopub.status.idle":"2022-09-02T16:38:35.802861Z","shell.execute_reply.started":"2022-09-02T16:37:32.621058Z","shell.execute_reply":"2022-09-02T16:38:35.801730Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nfrom torch.utils.data import DataLoader, random_split\nimport albumentations as A\n\nimport segmentation_models_pytorch as smp\n# import torchsummary\n\nimport pandas as pd\nimport numpy as np\nimport random, shutil, time, os\n\nimport sklearn\nimport cv2\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\nimport albumentations as A\n\nfrom glob import glob\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import KFold, GroupKFold\nfrom sklearn.metrics import roc_auc_score\n# from skimage import color\nfrom IPython import display as ipd\n\nimport scipy\nimport pdb\nimport gc\n\nimport monai\nimport tifffile as tiff\n\nfrom torch.cuda import amp\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint('done')","metadata":{"execution":{"iopub.status.busy":"2022-09-02T16:41:25.065241Z","iopub.execute_input":"2022-09-02T16:41:25.066279Z","iopub.status.idle":"2022-09-02T16:41:34.018561Z","shell.execute_reply.started":"2022-09-02T16:41:25.066234Z","shell.execute_reply":"2022-09-02T16:41:34.016664Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"done\n","output_type":"stream"}]},{"cell_type":"code","source":"CFG = {\n    'lr':6e-4,\n    'shape':(320, 320),\n}\n\nreduce = 4\n\n\nTRAIN = True\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2022-09-02T16:41:34.020710Z","iopub.execute_input":"2022-09-02T16:41:34.021618Z","iopub.status.idle":"2022-09-02T16:41:34.091289Z","shell.execute_reply.started":"2022-09-02T16:41:34.021575Z","shell.execute_reply":"2022-09-02T16:41:34.089488Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed=44):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    \ndef clear_cache():\n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-09-02T16:41:34.093141Z","iopub.execute_input":"2022-09-02T16:41:34.093525Z","iopub.status.idle":"2022-09-02T16:41:34.105563Z","shell.execute_reply.started":"2022-09-02T16:41:34.093487Z","shell.execute_reply":"2022-09-02T16:41:34.104562Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"BASE_DIR = '../input/hubmap-organ-segmentation'\n\nif TRAIN:\n    DATA_DIR = os.path.join(BASE_DIR, 'train_images')\nelse:\n    DATA_DIR = os.path.join(BASE_DIR, 'test_images')\n            \ndf = pd.read_csv(os.path.join(BASE_DIR, 'train.csv'))\ndf['path'] = df['id'].apply(lambda fname : os.path.join(DATA_DIR, str(fname) + '.tiff'))\norgan_to_class = {\n    'prostate':0,\n    'spleen':1,\n    'lung':2,\n    'kidney':3,\n    'largeintestine':4\n}\ndf['classes'] = df['organ'].apply(lambda organ : organ_to_class[organ])\ndf.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-09-02T16:41:34.107882Z","iopub.execute_input":"2022-09-02T16:41:34.108498Z","iopub.status.idle":"2022-09-02T16:41:34.709151Z","shell.execute_reply.started":"2022-09-02T16:41:34.108430Z","shell.execute_reply":"2022-09-02T16:41:34.708012Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"      id     organ data_source  img_height  img_width  pixel_size  \\\n0  10044  prostate         HPA        3000       3000         0.4   \n1  10274  prostate         HPA        3000       3000         0.4   \n2  10392    spleen         HPA        3000       3000         0.4   \n3  10488      lung         HPA        3000       3000         0.4   \n4  10610    spleen         HPA        3000       3000         0.4   \n\n   tissue_thickness                                                rle   age  \\\n0                 4  1459676 77 1462675 82 1465674 87 1468673 92 14...  37.0   \n1                 4  715707 2 718705 8 721703 11 724701 18 727692 3...  76.0   \n2                 4  1228631 20 1231629 24 1234624 40 1237623 47 12...  82.0   \n3                 4  3446519 15 3449517 17 3452514 20 3455510 24 34...  78.0   \n4                 4  478925 68 481909 87 484893 105 487863 154 4908...  21.0   \n\n      sex                                               path  classes  \n0    Male  ../input/hubmap-organ-segmentation/train_image...        0  \n1    Male  ../input/hubmap-organ-segmentation/train_image...        0  \n2    Male  ../input/hubmap-organ-segmentation/train_image...        1  \n3    Male  ../input/hubmap-organ-segmentation/train_image...        2  \n4  Female  ../input/hubmap-organ-segmentation/train_image...        1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>organ</th>\n      <th>data_source</th>\n      <th>img_height</th>\n      <th>img_width</th>\n      <th>pixel_size</th>\n      <th>tissue_thickness</th>\n      <th>rle</th>\n      <th>age</th>\n      <th>sex</th>\n      <th>path</th>\n      <th>classes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10044</td>\n      <td>prostate</td>\n      <td>HPA</td>\n      <td>3000</td>\n      <td>3000</td>\n      <td>0.4</td>\n      <td>4</td>\n      <td>1459676 77 1462675 82 1465674 87 1468673 92 14...</td>\n      <td>37.0</td>\n      <td>Male</td>\n      <td>../input/hubmap-organ-segmentation/train_image...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10274</td>\n      <td>prostate</td>\n      <td>HPA</td>\n      <td>3000</td>\n      <td>3000</td>\n      <td>0.4</td>\n      <td>4</td>\n      <td>715707 2 718705 8 721703 11 724701 18 727692 3...</td>\n      <td>76.0</td>\n      <td>Male</td>\n      <td>../input/hubmap-organ-segmentation/train_image...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10392</td>\n      <td>spleen</td>\n      <td>HPA</td>\n      <td>3000</td>\n      <td>3000</td>\n      <td>0.4</td>\n      <td>4</td>\n      <td>1228631 20 1231629 24 1234624 40 1237623 47 12...</td>\n      <td>82.0</td>\n      <td>Male</td>\n      <td>../input/hubmap-organ-segmentation/train_image...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10488</td>\n      <td>lung</td>\n      <td>HPA</td>\n      <td>3000</td>\n      <td>3000</td>\n      <td>0.4</td>\n      <td>4</td>\n      <td>3446519 15 3449517 17 3452514 20 3455510 24 34...</td>\n      <td>78.0</td>\n      <td>Male</td>\n      <td>../input/hubmap-organ-segmentation/train_image...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10610</td>\n      <td>spleen</td>\n      <td>HPA</td>\n      <td>3000</td>\n      <td>3000</td>\n      <td>0.4</td>\n      <td>4</td>\n      <td>478925 68 481909 87 484893 105 487863 154 4908...</td>\n      <td>21.0</td>\n      <td>Female</td>\n      <td>../input/hubmap-organ-segmentation/train_image...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# https://www.kaggle.com/paulorzp/rle-functions-run-length-encode-decode\ndef rle_encode(img):\n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels = img.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\n\ndef rle_decode(mask_rle, wid, hei):\n    shape = (wid, hei)\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape).T\n\n\ndef img_read(path):\n    img = tiff.imread(path)\n    return img\n\n\nclass Dataset2D(torch.utils.data.Dataset):\n    def __init__(self, df_sub, train=True):\n        self.train = train\n        \n        self.paths = np.array(df_sub['path'])\n        self.rles = np.array(df_sub['rle'])\n        self.classes = np.array(df_sub['classes'])\n        self.wid = np.array(df_sub['img_width'])\n        self.hei = np.array(df_sub['img_height'])\n        \n        \n    def __len__(self):\n        return len(self.paths)\n    \n    def transform(self, img, mask):\n        trans = A.Compose([\n#             A.ToFloat(max_value=65535.0), # essential because albu requires 32 bits!!! ONLY THIS can force it work with 16 bits!!\n\n            A.VerticalFlip(p=0.5),\n            A.HorizontalFlip(p=0.5),\n            A.RandomRotate90(p=0.5),\n\n\n            A.ShiftScaleRotate(\n                scale_limit=0.12,  # 0\n                shift_limit=0.02,  # 0.05\n                rotate_limit=15,\n                border_mode=cv2.BORDER_CONSTANT,\n                value=(1,1,1),\n                always_apply=True,\n                p=1,\n            ),\n\n            A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, always_apply=True),\n#             A.OneOf([\n#                     A.ElasticTransform(\n#                         alpha=1, \n# #                         sigma=25, \n#                         always_apply=True,\n#                     ),\n#                     A.GridDistortion(\n#                         always_apply=True,\n#                     ),\n#                     A.OpticalDistortion(\n#                         distort_limit=0.05, \n#                         shift_limit=0.05, \n#                         always_apply=True,\n#                     ),\n#                 ], p=0.3\n#             ),\n        ])\n        return trans(image=img, mask=mask)\n\n     \n    def data_prep_aug(self, img, mask, classes):\n        shape = CFG['shape']\n        img = (cv2.resize(img, shape, interpolation=cv2.INTER_AREA) / img.max()).astype('float32')\n        mask = (cv2.resize(mask, shape, interpolation=cv2.INTER_AREA)).astype('float32')\n        \n        if self.train:\n            trans = self.transform(img, mask)\n            img = trans['image']\n            mask = trans['mask']\n        \n        blank_mask = np.zeros((5, shape[0], shape[1]))\n        blank_mask[classes, :, :] = mask\n        mask = blank_mask\n\n        img = img.transpose(2,0,1)\n\n        return torch.tensor(img, dtype=torch.float16, device=device), torch.tensor(mask, dtype=torch.float16, device=device)\n    \n    def __getitem__(self, idx):\n        shape = CFG['shape']\n        img = img_read(self.paths[idx])\n        mask = rle_decode(self.rles[idx], self.wid[idx], self.hei[idx])\n\n        # data preprocessing and augmentation\n        img, masks = self.data_prep_aug(img, mask, self.classes[idx])\n        \n        return img, masks","metadata":{"execution":{"iopub.status.busy":"2022-09-02T16:41:34.711750Z","iopub.execute_input":"2022-09-02T16:41:34.712567Z","iopub.status.idle":"2022-09-02T16:41:34.735077Z","shell.execute_reply.started":"2022-09-02T16:41:34.712524Z","shell.execute_reply":"2022-09-02T16:41:34.733947Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# # CHECK INPUT CORRECTNESS\n# train_ds = Dataset2D(df, train=True)\n# train_ds_loader = torch.utils.data.DataLoader(train_ds, batch_size=1, num_workers=0)\n\n# for i, a in enumerate(train_ds_loader):\n#     if i < 3:\n#         print(a[1].shape)\n# #         print((a[1][0]).dtype)\n#         plt.figure(figsize=(10,10))\n# #         plt.subplot(1,2,1)\n#         plt.subplots()\n#         plt.imshow(a[0][0].cpu().detach().numpy().astype('float32').transpose(1,2,0))\n# #         plt.subplot(1,2,2)\n#         plt.subplots()\n#         plt.imshow(a[1][0][0].cpu().detach().numpy().astype('float32'))\n#     else:\n#         break\n\n","metadata":{"execution":{"iopub.status.busy":"2022-09-02T14:50:44.091786Z","iopub.execute_input":"2022-09-02T14:50:44.092149Z","iopub.status.idle":"2022-09-02T14:50:44.096996Z","shell.execute_reply.started":"2022-09-02T14:50:44.092117Z","shell.execute_reply":"2022-09-02T14:50:44.095690Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def imshow(img, return_only=False, pause=False, show_axis=True):\n    if isinstance(img, np.ndarray):\n        if len(img.shape) == 4:\n            img = img[0]\n        if img.shape[0] == 3:\n            img = img.transpose(2,0,1)\n    \n    if isinstance(img, torch.Tensor):\n        if len(img.shape) == 4:\n            img = img.cpu().detach()[0].numpy().transpose(2,0,1)\n        elif len(img.shape) == 3:\n            if img.shape[0] == 3:\n                img = img.cpu().detach().numpy().transpose(2,0,1)\n        elif len(img.shape) == 2:\n            img = img.cpu().detach().numpy()\n    \n    if return_only:\n        return img\n    else:\n#         plt.figure(figsize=(5,5))\n        plt.subplots()\n        plt.imshow(img)\n        if pause:\n            plt.pause(1)\n        if not show_axis:\n            plt.axis('off')","metadata":{"execution":{"iopub.status.busy":"2022-09-02T16:41:34.755323Z","iopub.execute_input":"2022-09-02T16:41:34.755596Z","iopub.status.idle":"2022-09-02T16:41:34.764298Z","shell.execute_reply.started":"2022-09-02T16:41:34.755569Z","shell.execute_reply":"2022-09-02T16:41:34.763273Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# idx = 62\n\n# def read(idx):\n#     img = cv2.imread(df['path'][idx], cv2.IMREAD_ANYDEPTH)\n#     shape = CFG['shape']\n#     img = (cv2.resize(img, shape, interpolation=cv2.INTER_AREA) / img.max()).astype('float32')\n    \n#     blank_mask = np.zeros((df.img_width[idx],  df.img_height[idx], 3))\n#     blank_mask[:, :, 0] = rle_decode(df.segmentation[idx][0], df.img_width[idx], df.img_height[idx])\n#     blank_mask[:, :, 1] = rle_decode(df.segmentation[idx][1], df.img_width[idx], df.img_height[idx])\n#     blank_mask[:, :, 2] = rle_decode(df.segmentation[idx][2], df.img_width[idx], df.img_height[idx])\n#     mask = blank_mask\n#     mask = cv2.resize(mask, shape, interpolation=cv2.INTER_AREA).astype('float32').transpose(2,0,1).reshape(1, 3, shape[0], shape[1])\n    \n#     blank_img = np.zeros((shape[0], shape[1], 3))\n#     blank_img[:, :, 0] = img\n#     blank_img[:, :, 1] = img\n#     blank_img[:, :, 2] = img\n#     img = blank_img.transpose(2,0,1).reshape((1, 3, shape[0], shape[1]))\n    \n#     return img, mask\n    \n\n# def predict(idx, model=model, to_numpy=True, log=True):\n#     img, mask = read(idx)\n    \n#     model.eval()\n\n#     mask = torch.tensor(mask, device=device,dtype=torch.float)\n#     img = torch.tensor(img, device=device,dtype=torch.float)\n#     pred = torch.sigmoid(model(img))\n\n#     pred[pred < 0.5] = 0\n#     pred[pred > 0.5] = 1\n    \n#     if log:\n#         dl = monai.losses.DiceLoss()(mask, pred)\n#         print((1-dl.cpu().detach().numpy()))\n\n\n#     if to_numpy:\n#         pred = pred.cpu().detach().numpy()\n#         img = img.cpu().detach().numpy()\n#         mask = mask.cpu().detach().numpy()\n\n#     return img, mask, pred\n# img, mask, pred = predict(idx)","metadata":{"execution":{"iopub.status.busy":"2022-08-31T02:42:35.565101Z","iopub.execute_input":"2022-08-31T02:42:35.565609Z","iopub.status.idle":"2022-08-31T02:42:35.574631Z","shell.execute_reply.started":"2022-08-31T02:42:35.565569Z","shell.execute_reply":"2022-08-31T02:42:35.573624Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"USE_WANDB = True\nif USE_WANDB:\n    import wandb\n    from wandb.keras import WandbCallback\n    secret_value = '5526656efba1b3f066b08df55e01dd2c5101c5ba'\n    wandb.login(key=secret_value)\n    \n    # wandb.init(project='unet_tract_tumor')","metadata":{"execution":{"iopub.status.busy":"2022-09-02T16:41:41.165739Z","iopub.execute_input":"2022-09-02T16:41:41.166333Z","iopub.status.idle":"2022-09-02T16:41:47.706180Z","shell.execute_reply.started":"2022-09-02T16:41:41.166295Z","shell.execute_reply":"2022-09-02T16:41:47.705094Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"}]},{"cell_type":"code","source":"# UNet EFFB7\nactivation = None\nmodel = smp.Unet(\n    encoder_name='efficientnet-b7',\n    decoder_channels= (512, 256, 128, 64, 32),\n    decoder_use_batchnorm=True,\n    activation=activation,\n    in_channels=3,\n    classes=5,\n)\nmodel = model.to(device)\nprint('finished')","metadata":{"execution":{"iopub.status.busy":"2022-09-02T16:41:47.708218Z","iopub.execute_input":"2022-09-02T16:41:47.708627Z","iopub.status.idle":"2022-09-02T16:42:02.437845Z","shell.execute_reply.started":"2022-09-02T16:41:47.708587Z","shell.execute_reply":"2022-09-02T16:42:02.436690Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b7-dcc49843.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b7-dcc49843.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/254M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"376c1171b56b42ee8afdc7d76707ac54"}},"metadata":{}},{"name":"stdout","text":"finished\n","output_type":"stream"}]},{"cell_type":"code","source":"kf = KFold(n_splits=10)\nfor train_ind, val_ind in kf.split(df, df['classes'], groups=None):\n    print(len(train_ind))\n    break","metadata":{"execution":{"iopub.status.busy":"2022-08-31T16:34:38.795701Z","iopub.execute_input":"2022-08-31T16:34:38.796101Z","iopub.status.idle":"2022-08-31T16:34:38.805429Z","shell.execute_reply.started":"2022-08-31T16:34:38.796066Z","shell.execute_reply":"2022-08-31T16:34:38.802887Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"315\n","output_type":"stream"}]},{"cell_type":"code","source":"clear_cache()","metadata":{"execution":{"iopub.status.busy":"2022-08-31T16:36:24.608557Z","iopub.execute_input":"2022-08-31T16:36:24.608914Z","iopub.status.idle":"2022-08-31T16:36:24.880567Z","shell.execute_reply.started":"2022-08-31T16:36:24.608882Z","shell.execute_reply":"2022-08-31T16:36:24.879290Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"kf = KFold(n_splits=10)\n\n# model = monai.networks.nets.UNet(\n#     spatial_dims=2,\n#     in_channels=3,\n#     out_channels=3,\n\n#     channels=(32, 64, 128, 256, 512),\n# #     strides=(2, 2, 2, 2),\n#     num_res_units=2,\n# )\n\nepochs = 30\ntrain_bs = 8\n\nnum_epoch_2_skip = 0\n\nn_accumulate = 2\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=CFG['lr'])\ncriterion = lambda y_pred, y_true : monai.losses.FocalLoss()(y_pred, y_true) + torch.log(torch.cosh(monai.losses.DiceLoss(sigmoid=(activation == None))(y_pred, y_true)))\n# criterion = lambda y_pred, y_true : 1 - dice_coef(y_pred, y_true)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=int(epochs*39/n_accumulate), eta_min=5e-6)\n\n# train_load * epoches * \n# \nfor fold, (train_ind, val_ind) in tqdm(enumerate(kf.split(df, df['classes'], groups=None)), desc='Train '):\n\n    EXP_NAME = f\"PostRUNS_fold{fold}\"\n    \n    if USE_WANDB:\n        wandb.init(name = EXP_NAME, project=\"HACKING_v1\", entity=\"kagglers\", \n                    config = CFG, save_code = False, group = \"UNet EffB7 LOGCOSH\",notes='_6e-4_320_39, normal DA, normal scheduler, but 6e-4'\n                  )\n        wandb.watch(model, log=None)\n\n\n    best_val_dice = -1\n    for epoch in range(1, epochs+1):\n\n        clear_cache()\n        clear_cache()\n        clear_cache()\n        \n        seed_everything(44)\n        val_dice = 0\n        if epoch <= num_epoch_2_skip:\n            continue\n            \n        print(f'Epoch {epoch}/{epochs}', end='')\n\n        model.train()\n        scaler = torch.cuda.amp.GradScaler(init_scale=65536.0)\n\n        dataset_size = 0\n        running_loss = 0.0\n        \n        train_ds = Dataset2D(df.iloc[train_ind], train=True)\n        train_ds_loader = torch.utils.data.DataLoader(train_ds, batch_size=train_bs)\n        \n        val_ds = Dataset2D(df.iloc[val_ind], train=False)\n        val_ds_loader = torch.utils.data.DataLoader(val_ds, batch_size=2)\n\n        \n        # ========================================\n        # TRAINING\n        # ========================================\n\n        pbar = tqdm(enumerate(train_ds_loader), total=len(train_ds_loader), desc='Train ')\n        \n        for step, (images, masks) in pbar:\n            batch_size = images.size(0)\n\n            with amp.autocast(enabled=True):\n                y_pred = model(images)\n                loss   = criterion(y_pred, masks)\n\n            (scaler.scale(loss)/n_accumulate).backward()\n\n            if (step + 1) % n_accumulate == 0:\n#                 torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n                scaler.step(optimizer)\n                scaler.update()\n\n                optimizer.zero_grad()\n\n                if scheduler is not None:\n                    scheduler.step()\n\n            running_loss += (loss.item() * batch_size)\n            current_loss = (loss.item())\n            dataset_size += batch_size\n\n            epoch_loss = running_loss / dataset_size\n            current_lr = optimizer.param_groups[0]['lr']\n            \n\n            if np.isnan(epoch_loss):\n                print('NAN LOSS')\n                break\n            \n            if USE_WANDB:\n                wandb.log({\n                    'train_current_loss':current_loss,\n                    'lr':current_lr,\n                })\n                \n            pbar.set_postfix(\n                train_loss=f'{epoch_loss:0.4f}',\n                current_loss=f'{current_loss:0.5f}',\n                lr=f'{current_lr:0.6f}',\n            )\n        if USE_WANDB:\n            wandb.log({\n                'train_epoch_loss':epoch_loss,\n            })\n        torch.save(model.state_dict(), 'running_pt.pt')\n        \n        \n        clear_cache()\n        clear_cache()\n        clear_cache()\n        \n        \n        # ========================================\n        # Validation\n        # ========================================\n        \n        model.eval()\n        \n        dataset_size = 0\n        running_loss = 0.0\n        \n        pbar = tqdm(enumerate(val_ds_loader), total=len(val_ds_loader), desc='Valid ')\n\n        for step, (images, masks) in pbar:  \n            images = images.to(dtype=torch.float, device=device)\n            masks = masks.to(dtype=torch.float, device=device)\n            \n            batch_size = images.size(0)\n\n            y_pred  = model(images)\n            loss    = criterion(y_pred, masks)\n\n            running_loss += (loss.item() * batch_size)\n            dataset_size += batch_size\n\n            epoch_loss = running_loss / dataset_size\n\n            y_pred = nn.Sigmoid()(y_pred)\n            \n            y_pred[y_pred < 0.5] = 0\n            y_pred[y_pred > 0.5] = 1\n            y_pred[y_pred == 0.5] = 1\n            \n            dl = monai.losses.DiceLoss()(masks, y_pred)\n            val_dice += (1-dl.cpu().detach().numpy())\n#             val_dice = dice_coef(masks, y_pred).cpu().detach().numpy()\n            \n            current_lr = optimizer.param_groups[0]['lr']\n            if USE_WANDB:\n                wandb.log({\n                    'running_valid_loss':epoch_loss,\n                })\n            pbar.set_postfix(\n                valid_loss=f'{epoch_loss:0.4f}',\n                dice_acc=f'{val_dice}'\n            )\n            \n        if USE_WANDB:\n            wandb.log({\n                'val_dice':val_dice,\n                'valid_loss':epoch_loss,\n            })\n                \n        if val_dice > best_val_dice:\n            best_val_dice = val_dice\n            print('saving...')\n            torch.save(model.state_dict(), f'{EXP_NAME}_{epoch}.pt')\n                \n    if USE_WANDB:\n        wandb.finish()\n    break","metadata":{"execution":{"iopub.status.busy":"2022-09-02T16:42:09.568419Z","iopub.execute_input":"2022-09-02T16:42:09.568768Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Train : 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb1aa96507bd46a6a7b0a1965fa6f631"}},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtonycfakename\u001b[0m (\u001b[33mkagglers\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.13.2 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.12.21"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20220902_164209-yifssuvs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href=\"https://wandb.ai/kagglers/HACKING_v1/runs/yifssuvs\" target=\"_blank\">PostRUNS_fold0</a></strong> to <a href=\"https://wandb.ai/kagglers/HACKING_v1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/30","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train :   0%|          | 0/40 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecd328747bb147c4a40f500a5ba30f5e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Valid :   0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54b3189f6c0f4ee5ab7d71e35e446ed2"}},"metadata":{}},{"name":"stdout","text":"saving...\nEpoch 2/30","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train :   0%|          | 0/40 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"176e9c7a84634a60b97cf3c0b44870a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Valid :   0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc370c7f240a4b0bad48376c99c86ad6"}},"metadata":{}},{"name":"stdout","text":"saving...\nEpoch 3/30","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train :   0%|          | 0/40 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecf614710e704e09a13135b800b4d61d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Valid :   0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a94a348b84c4e8a94664d9c349ab9fa"}},"metadata":{}},{"name":"stdout","text":"Epoch 4/30","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train :   0%|          | 0/40 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99fd51397bbd4314ac9bd2adc15216ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Valid :   0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e29d13da42a4398ab7b041a473450a4"}},"metadata":{}},{"name":"stdout","text":"Epoch 5/30","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train :   0%|          | 0/40 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0caa81ccf3bc43c592dbf8b6fd30db2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Valid :   0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c204c8e0d454e19a3524fda413809e1"}},"metadata":{}},{"name":"stdout","text":"Epoch 6/30","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train :   0%|          | 0/40 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9af4e20d68084de696e70e65400b031d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Valid :   0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6195ec7092c404c845d708fbe262135"}},"metadata":{}},{"name":"stdout","text":"Epoch 7/30","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train :   0%|          | 0/40 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"829e6dbcf9b4422089c160c7e84f26a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Valid :   0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4616c59a1ce48ad9cccb7b285f4a014"}},"metadata":{}},{"name":"stdout","text":"Epoch 8/30","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train :   0%|          | 0/40 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ca9bd2c01e6418195c2d3cd22c67ec5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Valid :   0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c30471bc72ea45d28c7b9f3562fea155"}},"metadata":{}},{"name":"stdout","text":"Epoch 9/30","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train :   0%|          | 0/40 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"183b71620c53455eb38b933392532dab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Valid :   0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca3b04b5b58f4c578d6802f02063379a"}},"metadata":{}},{"name":"stdout","text":"saving...\nEpoch 10/30","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train :   0%|          | 0/40 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3c70b23d787492d9d285860b1be9760"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Valid :   0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd7762e6c73e440da80476bfb1a43775"}},"metadata":{}},{"name":"stdout","text":"Epoch 11/30","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train :   0%|          | 0/40 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d1915de6167405f986a4e39bce0b1f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Valid :   0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9080f2eff6804ce3bb56147779a8dc37"}},"metadata":{}},{"name":"stdout","text":"saving...\nEpoch 12/30","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train :   0%|          | 0/40 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aabef90a965d4fde86ee1c78e4ab23f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Valid :   0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16499c151b744e43bbdc396979b4a08c"}},"metadata":{}},{"name":"stdout","text":"Epoch 13/30","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train :   0%|          | 0/40 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90c145db95544f038a5db7f20cd1ec8f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Valid :   0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e9c78ba3eed4ccbb9a17f73f744e9dc"}},"metadata":{}},{"name":"stdout","text":"Epoch 14/30","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train :   0%|          | 0/40 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c258f61f4a6b4e819550ce6e9403340c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Valid :   0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08d13994b8c248df9b180464b3ff316e"}},"metadata":{}}]},{"cell_type":"code","source":"clear_cache()","metadata":{"execution":{"iopub.status.busy":"2022-09-02T14:55:06.836616Z","iopub.execute_input":"2022-09-02T14:55:06.837636Z","iopub.status.idle":"2022-09-02T14:55:07.252817Z","shell.execute_reply.started":"2022-09-02T14:55:06.837594Z","shell.execute_reply":"2022-09-02T14:55:07.251628Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"l = []\n\nval_ds = Dataset2D(df, train=False)\nval_ds_loader = torch.utils.data.DataLoader(val_ds, batch_size=1)\n\n# model.load_state_dict(torch.load('../input/hacking-uneteff7-baseline/PostRUNS_fold0_10.pt'))\nmodel.eval()\nclear_cache()\nfor images, labels in val_ds_loader:\n    images = images.to(dtype=torch.float, device=device)\n\n    with torch.no_grad():\n        y_pred  = model(images)\n    y_pred = nn.Sigmoid()(y_pred)\n            \n    y_pred[y_pred < 0.5] = 0\n    y_pred[y_pred > 0.5] = 1\n    \n    l.append(y_pred)\n    break\n    \n    \n","metadata":{"execution":{"iopub.status.busy":"2022-08-31T02:57:24.728903Z","iopub.execute_input":"2022-08-31T02:57:24.729293Z","iopub.status.idle":"2022-08-31T02:57:25.193632Z","shell.execute_reply.started":"2022-08-31T02:57:24.729261Z","shell.execute_reply":"2022-08-31T02:57:25.192661Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"for i in range(len(l)):\n    e = l[i][0].detach().cpu().numpy()\n\n    c = e[np.argmax(np.sum(e, axis=(1,2)))]\n    \n    c[c < 0.5] = 0\n    c[c > 0.5] = 1\n    \n    plt.imshow(c)\n    c = cv2.resize(c, (df.iloc[i]['img_width'], df.iloc[i]['img_height']))\n        \n    if np.sum(c) < 100:\n        rle = rle_encode(np.ones((df.iloc[i]['img_width'], df.iloc[i]['img_height'])))\n    else:\n        rle = rle_encode(c)","metadata":{"execution":{"iopub.status.busy":"2022-08-31T03:23:38.942489Z","iopub.execute_input":"2022-08-31T03:23:38.943021Z","iopub.status.idle":"2022-08-31T03:23:39.040943Z","shell.execute_reply.started":"2022-08-31T03:23:38.942897Z","shell.execute_reply":"2022-08-31T03:23:39.039243Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_17/251741915.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'l' is not defined"],"ename":"NameError","evalue":"name 'l' is not defined","output_type":"error"}]}]}